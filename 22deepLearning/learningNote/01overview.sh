auther:
        zouxy09@qq.com
        http://blog.csdn.net/zouxy09/article/details/8775360

2006 提出Deep learning

机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题:
    例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等
    机器学习解决问题思路:
        传感器（例如CMOS）来获得数据, 然后经过预处理、特征提取、特征选择，
            再到推理、预测或者识别。
        手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，
            能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间
        Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature
            Learning 不要人参与特征的选取过程。

关于特征:
        特征表示的粒度
            从不同的抽象层次看问题
        初级（浅层）特征表示:
            稀疏编码:
                http://blog.csdn.net/zouxy09/article/details/8775488
                复杂图形，往往由一些基本结构组成 
                    声音也是 其余的声音可以由20种基本结构合成
        结构性特征表示:
                层层抽象
        需要有多少个特征:
                任何一种方法，特征越多，给出的参考信息就越多，准确性会得到
                提升。但特征多意味着计算复杂，探索的空间大，可以用来训练的
                数据在每个特征上就会稀疏，都会带来各种问题，并不一定特
                征越多越好。

Deep Learning的基本思想:
        信息论中有个“信息逐层丢失”的说法（信息处理不等式），设处理a信
        息得到b，再对b处理得到c，那么可以证明：a和c的互信息不会超过a和b的
        互信息。 
        对于深度学习来说，其思想就是对堆叠多个层，也就是说这一层的输出作
        为下一层的输入。通过这种方式，就可以实现对输入信息进行分级表达了。

浅层学习（Shallow Learning）和深度学习（Deep Learning）:
        20世纪80年代末期，用于人工神经网络的反向传播算法（也叫Back 
        Propagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基
        于统计模型的机器学习热潮。这个热潮一直持续到今天。这个时候的人工
        神经网络，虽也被称作多层感知机（Multi-layer Perceptron），但实际
        是种只含有一层隐层节点的浅层模型。

          20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机
          （SVM，Support Vector Machines）、 Boosting、最大熵方法（如LR，
          Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层
          节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在
          理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的
          难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络
          反而相对沉寂。 
    深度学习是机器学习的第二次浪潮
        2006年，加拿大多伦多大学教授:在《科学》上发表了一篇文章
            两个主要观点：1）多隐层的人工神经网络具有优异的特征学习能力，
            学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；
            2）深度神经网络在训练上的难度，可以通过“逐层初始化”
            （layer-wise pre-training）来有效克服，在这篇文章中，逐层初始
            化是通过无监督学习实现的。
        区别于传统的浅层学习，深度学习的不同在于：
            1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；
            2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，
                将样本在原空间的特征表示变换到一个新特征空间，从而使分类
                或预测更加容易。与人工规则构造特征的方法相比，利用大数据
                来学习特征，更能够刻画数据的丰富内在信息。
Deep learning与Neural Network:
        Deep learning本身算是machine learning的一个分支，简单可以理解为
            neural network的发展。
        neural network曾经是ML领域特别火热的一个方向，但是后来确慢慢淡出了
            ，原因包括以下几个方面：
            1）比较容易过拟合，参数比较难tune，而且需要不少trick；
            2）训练速度比较慢，在层次比较少（小于等于3）的情况下效果并不比
                其它方法更优
    相同不同点:
        二者的相同在于deep learning采用了神经网络相似的分层结构，系统由包
        括输入层、隐层（多层）、输出层组成的多层网络，只有相邻层节点之间
        有连接，同一层以及跨层节点之间相互无连接，每一层可以看作是一个
        logistic regression模型；这种分层结构，是比较接近人类大脑的结构的。

        而为了克服神经网络训练中的问题，DL采用了与神经网络很不同的训练机制。
        传统神经网络中，采用的是back propagation的方式进行，简单来讲就是采用
        迭代的算法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据
        当前输出和label之间的差去改变前面各层的参数，直到收敛（整体是一个梯
        度下降法）。而deep learning整体上是一个layer-wise的训练机制。这样做
        的原因是因为，如果采用back propagation的机制，对于一个deep network
        （7层以上），残差传播到最前面的层已经变得太小，出现所谓的gradient
        diffusion（梯度扩散）

八、Deep learning训练过程:
        8.1、传统神经网络的训练方法为什么不能用在深度神经网络:
            BP算法存在的问题:
                （1）梯度越来越稀疏：从顶层越往下，误差校正信号越来越小；
                （2）收敛到局部最小值：尤其是从远离最优区域开始的时候
                    （随机值初始化会导致这种情况的发生）；
                （3）一般，我们只能用有标签的数据来训练：但大部分的数据是没
                    标签的，而大脑可以从没有标签的的数据中学习；
        deep learning训练过程:
            如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差
            就会逐层传递。这会面临跟上面监督学习中相反的问题，会严重欠拟合
            （因为深度网络的神经元和参数太多了）

            hinton提出了在非监督数据上建立多层神经网络的一个有效方法，简单
            的说，分为两步
                一是每次训练一层网络
                二是调优，使原始表示x向上生成的高级表示r和该高级表示r向下
                    生成的x'尽可能一致。方法是：'
                1）首先逐层构建单层神经元，这样每次都是训练一个单层网络。
                2）当所有层训练完后，Hinton使用wake-sleep算法进行调优。

                将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个
                单层神经网络，而其它层则变为了图模型。向上的权重用于“认知”
                向下的权重用于“生成”。然后使用Wake-Sleep算法调整所有的权重
                让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能
                正确的复原底层的结点。比如顶层的一个结点表示人脸，那么所有
                人脸的图像应该激活这个结点，并且这个结果向下生成的图像应该
                能够表现为一个大概的人脸图像。Wake-Sleep算法分为醒（wake）
                和睡（sleep）两个部分。

                1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重
                ）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层
                间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，
                改变我的权重使得我想象的东西就是这样的”。

                2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和
                向下权重，生成底层的状态，同时修改层间向上的权重。也就是
                “如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得
                这种景象在我看来就是这个概念”。
    deep learning训练过程具体如下：:
        1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：

       采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是
       一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看
       作是feature learning过程）：

       具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层
       可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），
       由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数
       据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层
       后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；

        2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下
        传输，对网络进行微调）：

       基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是
       一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的
       第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值
       更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程
       度上归功于第一步的feature learning过程。

九、Deep Learning的常用模型或者方法:
        AutoEncoder自动编码器
            给定无标签数据，用非监督学习学习特征
                input输入一个encoder编码器，就会得到一个code，这个code也就是
                输入的一个表示，那么我们怎么知道这个code表示的就是input呢？
                我们加一个decoder解码器，这时候decoder就会输出一个信息，那么
                如果输出的这个信息和一开始的输入信号input是很像的（理想情况
                下就是一样的），那很明显，我们就有理由相信这个code是靠谱的。
                所以，我们就通过调整encoder和decoder的参数，使得重构误差最小
                ，这时候我们就得到了输入input信号的第一个表示了，也就是编码
                code了。因为是无标签数据，所以误差的来源就是直接重构后与原输
                入相比得到。
            2）通过编码器产生特征，然后训练下一层。这样逐层训练：
                那上面我们就得到第一层的code，我们的重构误差最小让我们相信
                这个code就是原输入信号的良好表达了，或者牵强点说，它和原信
                号是一模一样的（表达不一样，反映的是一个东西）。那第二层和第
                一层的训练方式就没有差别了，我们将第一层输出的code当成第二层
                的输入信号，同样最小化重构误差，就会得到第二层的参数，并且得
                到第二层输入的code，也就是原输入信息的第二个表达了。其他层就
                同样的方法炮制就行了（训练这一层，前面层的参数都是固定的，并
                且他们的decoder已经没用了，都不需要了）
            3）有监督微调：
                    到这里，这个AutoEncoder还不能用来分类数据，因为它还没有
                学习如何去连结一个输入和一个类。它只是学会了如何去重构或者复
                现它的输入而已。或者说，它只是学习获得了一个可以良好代表输入
                的特征，这个特征可以最大程度上代表原输入信号。那么，为了实现
                分类，我们就可以在AutoEncoder的最顶的编码层添加一个分类器（
                例如罗杰斯特回归、SVM等），然后通过标准的多层神经网络的监督
                训练方法（梯度下降法）去训练。
                这也分两种，一个是只调整分类器:
                另一种：通过有标签样本，微调整个系统：（如果有足够多的数据，
                这个是最好的。end-to-end learning端对端学习）
                   在研究中可以发现，如果在原有的特征中加入这些自动学习得到
                   的特征可以大大提高精确度，甚至在分类问题中比目前最好的分
                   类算法效果还要好！
        AutoEncoder存在一些变体，这里简要介绍下两个：
            Sparse AutoEncoder稀疏自动编码器：
                当然，我们还可以继续加上一些约束条件得到新的Deep Learning方
                法，如：如果在AutoEncoder的基础上加上L1的Regularity限制（
                L1主要是约束每一层中的节点中大部分都要为0，只有少数不为0，
                这就是Sparse名字的来源），我们就可以得到Sparse AutoEncoder法
            Denoising AutoEncoders降噪自动编码器：
                降噪自动编码器DA是在自动编码器的基础上，训练数据加入噪声，所
                以自动编码器必须学习去去除这种噪声而获得真正的没有被噪声污染
                过的输入。因此，这就迫使编码器去学习输入信号的更加鲁棒的表达
                ，这也是它的泛化能力比一般编码器强的原因。DA可以通过梯度下降
                算法去训练。
        9.2、Sparse Coding稀疏编码
            通俗的说，就是将一个信号表示为一组基的线性组合，而且要求只需要较
            少的几个基就可以将信号表示出来。“稀疏性”定义为：只有很少的几个
            非零元素或只有很少的几个远大于零的元素。要求系数 ai 是稀疏的意思
            就是说：对于一组输入向量，我们只想有尽可能少的几个系数远大于零。
            选择使用具有稀疏性的分量来表示我们的输入数据是有原因的，因为绝大
            多数的感官数据，比如自然图像，可以被表示成少量基本元素的叠加，在
            图像中这些基本元素可以是面或者线。同时，比如与初级视觉皮层的类比
            过程也因此得到了提升（人脑有大量的神经元，但对于某些图像或者边缘
            只有很少的神经元兴奋，其他都处于抑制状态）。

            稀疏编码算法是一种无监督学习方法，它用来寻找一组“超完备”基向
            量来更高效地表示样本数据。虽然形如主成分分析技术（PCA）能使我们
            方便地找到一组“完备”基向量，但是这里我们想要做的是找到一组“
            超完备”基向量来表示输入向量（也就是说，基向量的个数比输入向量的
            维数要大）。超完备基的好处是它们能更有效地找出隐含在输入数据内部
            的结构与模式。然而，对于超完备基来说，系数ai不再由输入向量唯一确
            定。因此，在稀疏编码算法中，我们另加了一个评判标准“稀疏性”来
            解决因超完备而导致的退化（degeneracy）问题

       Sparse coding分为两个部分：
            1）Training阶段：给定一系列的样本图片[x1, x 2, …]，我们需要学习
                得到一组基[Φ1, Φ2, …]，也就是字典。
            稀疏编码是k-means算法的变体，其训练过程也差不多（EM算法的思想：
            如果要优化的目标函数包含两个变量，如L(W, B)，那么我们可以先固定
            W，调整B使得L最小，然后再固定B，调整W使L最小，这样迭代交替，不
            断将L推向最小值。EM算法可以见我的博客：“从最大似然到EM算法浅解
            ”）。
            a）固定字典Φ[k]，然后调整a[k]，使得上式，即目标函数最小（即
                    解LASSO问题）。
            b）然后固定住a [k]，调整Φ [k]，使得上式，即目标函数最小
                    （即解凸QP问题）。
        不断迭代，直至收敛。这样就可以得到一组可以良好表示这一系列x
                的基，也就是字典。
        2）Coding阶段：给定一个新的图片x，由上面得到的字典，通过解一个
        LASSO问题得到稀疏向量a。这个稀疏向量就是这个输入向量x的一个
            稀疏表达了。
    9.3、Restricted Boltzmann Machine (RBM)限制波尔兹曼机
    9.4、Deep Belief Networks深信度网络
    9.5、Convolutional Neural Networks卷积神经网络
            它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复
            杂度，减少了权值的数量。该优点在网络的输入是多维图像时表现的更为
            明显，使图像可以直接作为网络的输入，避免了传统识别算法中复杂的特
            征提取和数据重建过程。卷积网络是为识别二维形状而特殊设计的一个多
            层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具
            有高度不变性。

            CNNs是第一个真正成功训练多层网络结构的学习算法。它利用空间关系减
            少需要学习的参数数目以提高一般前向BP算法的训练性能。CNNs作为一个
            深度学习架构提出是为了最小化数据的预处理要求。在CNN中，图像的一
            小部分（局部感受区域）作为层级结构的最低层的输入，信息再依次传输
            到不同的层，每层通过一个数字滤波器去获得观测数据的最显著的特征。
            这个方法能够获取对平移、缩放和旋转不变的观测数据的显著特征，因
            为图像的局部感受区域允许神经元或者处理单元可以访问到最基础的特征
            例如定向边缘或者角点。
        卷积神经网络的网络结构:
            卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而
            每个平面由多个独立神经元组成。
        3）关于参数减少与权值共享
            http://blog.csdn.net/zouxy09/article/details/8781543

    问题:
        http://blog.csdn.net/zouxy09/article/details/8782018

        

    

    

    

        

                

        

        

                

            

                

            

    

                

            

            

                

        


    











