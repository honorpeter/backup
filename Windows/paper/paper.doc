



	基于yolov2算法的深度学习目标识别
摘要：
	我们基于yolov2算法进行目标识别，yolov2是yolo的改进版。yolo是一种
把目标识别问题转化为回归问题的深度学习方法，有别于rcnn系列目标识别算法，也有别于基于DPM的传统目标识别算法。rcnn系列算法 采取proposal+分类的思路，proposal提供位置信息，分类对proposal进行分类。DPM是一种基于 提取HOG特征+使用窗口滑动HOG特征+分类窗口 的目标识别算法。yolo使用卷积神经网络进行图片的特征提取，在输出层回归bounding box的位置和box的类别。yolov2在yolo的基础上，提出了一系列改进的方法，在处理速度不变的情况下，提高了检测的精度。我们主要改进了yolov2的网络结构，去掉一些冗余的中间层，加快识别的速度，精度基本可以保持一致。

介绍
	人类只需要看一眼，便可以立马识别出图像中的物体是什么，并且知道他们在哪里。人类的视力系统识别效率如此的高和快速，可以让我们很快识别出复杂的物体。快速，高效的目标检测算法可以让计算机实时识别出视频中各种物体，如车辆，行人，可以作为了机器人的视觉系统。
	现在目标识别系统，都是基于分类来做的。为了识别目标，需要使用固定大小的窗口滑动整个图片，使用分类模型判断每个窗口所属类别的自信度，DPM目标识别模型便是依据这样的思路。rcnn系列模型先产生很多后选框，分类每个后选框，最后通过预测的张量调整每个后选框的位置来得出最后的结果。
	yolo将目标识别任务作为回归问题来处理，直接根据图像的像素值来预测目标区域的位置以及所属类别的概率。只需要扫描一次图片便可以识别出目标。检测过程如图1，相比较其他目标识别方法，yolo具有以下优点。






			图1： 目标识别过程： 1）将图片调整为448*448， 2） 使用卷积网络提取特征 3）通过模型得出的自信度来识别目标。
	1）检测速度快，可以实时检测目标，在Titan X GPU上可以跑45帧每秒，并且识别效果是其他实时识别的两倍。
	2）滑动窗口和列举后选框都是基于局部信息来进行分类，经常会把背景识别为目标，yolo通过全局图像信息来识别目标，可以减少背景的错误识别率。
	3）yolo泛化能力强，当预测没有训练过的目标时，效果会好些。


yolov2在yolo的基础上，提出了一系列方法改进了，在保持原有的速度的同时，提高了识别精度。主要在一下方面做了改进：
　　　1）把图片输入分辨率改为416 * 416，目的是让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。统计发现大物体通常占据了图像的中间位置，可以只用一个中心的cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。
	1）在每个的卷基层后面添加了batch normalization层，来归一化中间数据，使得输入数据，中间数据分布大致相同。通过添加batch normalization层，极大的加快了收敛速度。
	2） 提高了输入图片的分辨率，AlexNet输入图片会被resize到不足256 * 256，这导致分辨率不够高，给检测带来困难。yolo使用分辨率为448*448的图片作为输入。
	3）借鉴了Faster R-CNN中的anchor思想，在卷积特征图上进行滑窗采样，每个中心预测5种不同大小和比例的后选框。由于都是卷积不需要reshape，很好的保留的空间信息，最终特征图的每个特征点和原图的每个cell一一对应。而且用预测相对偏移（offset）取代直接预测坐标简化了问题，方便网络学习。

我们改进了网络结构，去除一些冗余的层。主要将几个卷基层去掉了。直接训练检测网络，没有先训练分类网络，再微调检测网络，取得的效果基本和yolov2保持一致。速度更快，权重模型从200M降到48M，速度更快。


使用卷积层产生候选框
YOLOv1使用全连接层的输出进行bounding box的预测，即要把1470*1的全链接层reshape为7*7*30特征），这样导致丢失了很多空间信息，定位不准去。YOLOv2借鉴了Faster R-CNN中的anchor思想： 简单理解为 在卷积特征图上进行滑窗采样，每个中心预测9种不同大小和比例的候选框。由于卷基层是直接预测，不需要reshape，很好的保留了空间信息，最终特征图的每个特征点和原图的每个cell一一对应。如图2



维度聚类
Yolov2借鉴了anchor的思想，Faster-RCNN中anchor boxes的个数和宽高维度是手动精选的先验框。为了能够选择了更好的、更有代表性的boxes维度，yolov2使用K-means聚类方法，通过对数据集中的ground true box做聚类，找到ground true box的统计规律。以聚类个数k为anchor boxs个数，以k个聚类中心box的宽高维度为anchor box的维度。
聚类的真正想要的是产生好的IOU得分的boxes。因此采用了如下距离度量：
　　　d(box; centroid) = 1 - IOU(box; centroid) 
聚类结果如图4：

图4： 随着k的增大，IOU也在增大，但是复杂度也在增加。所以平衡复杂度和IOU之后，最终得到k值为5。

　　　




数据集：
　　　Voc2012 voc2007
　　　




将输入图片调整为448*448大小，图片分割为7×7的网格




摘要  大致写的什么
	贡献点 点一下

2 具体介绍一下
	贡献点 展开
数据集	
3 怎么实现 caffe

4 实验效果

5 总结



参考文献：
压缩的
yolo一二
rcnn系列
dpm
google net
alxnet 
Voc数据集
batch normalization